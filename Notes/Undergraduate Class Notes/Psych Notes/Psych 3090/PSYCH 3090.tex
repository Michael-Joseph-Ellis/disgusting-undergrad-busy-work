\documentclass[11pt]{report}

\input{../../../Notes Template/preamble.tex}
\input{../../../Notes Template/macros.tex}
\input{../../../Notes Template/letterfonts.tex}
% for info on how to make tables https://www.overleaf.com/learn/latex/Tables
\usepackage{pgfplots}
\usepackage{multirow}
\usepackage{multicol}

\title{\Huge{Psych 3090}\\Intro To Experimental Psychology}
\author{\huge{Michael Joseph Ellis}}
\date{}

\begin{document}

\maketitle
\newpage
\tableofcontents
\pagebreak

\chapter{Introduction to Statistics}

\section{Introduction}

\chapterclaim{3 Purporses of Statistics}
{
    1. Describe - "Descriptive statistics" (First part of the semester)
    
    2. Make inferences - "Inferential statistics" (Second part of the semester)
    
    3. Communicate effectively - How to describe the data that you've got. (Throughout the semester)
}

\subsubsection{Why is learning stats important?}
\noindent Stages of scientific research: 
\begin{itemize}
    \item State the question
    \item Develop a hypothesis 
    \item Define variables
    \item Observe
    \item Analyze
    \item Make conclusions, refine theory, repeat
\end{itemize}
However, to which of these levels is knowledge of stats relevant?
\textbf{Statistics is relevant at all stages of scientific research.}
\newline\newline
\noindent\underline{Understanding stats\dots}
\begin{itemize}
    \item Is crucial when \underline{doing research}. 
    \item Is crucial when \underline{reading} others' research.
    \item Helps you \underline{think critically about data}.
    \item Is marketable! Survival in the information economy\dots
\end{itemize}

\subsubsection{The Big Picture: 4 Key Terms}
\chapterdefinition{The Big Picture: 4 Key Statistical Terms}
{
    \begin{itemize}
        \item Population - The entire group of interest. 
        \item Parameters describe populations. These parameters are often symbolized by greek letters.
        \item Sample - A subset of the population.
        \item Statistics describe samples. These statistics are often symbolized by english letters.
    \end{itemize}
    \textit{Note: An inference is a study from a sample going to the population. i.e., "I have a sample of 100 people, and I want to make a statement about the population of 1000 people."}
}

\subsubsection{3 Kinds of Variables}
\chapterdefinition{3 Kinds of Variables}
{
    \begin{itemize}
        \item Dependant Variables - What we measure in an experiment.
        \item Independant Variables - What we manipulate in an experiment.
        \item Extraneous Variables - Any variable that is neither a DV or an IV. 
    \end{itemize}
    \textit{Note: \underline{Memory Tip 1:} "The \underline{dependant} variable \underline{depends} on the independant variable."} \newline
    \textit{Note: \underline{Memory Tip 2:} "The \underline{d}ependant variable is your \underline{d}ata."}
}

\subsubsection{Correlations vs. Experiements} 

\textbf{Correlations}
\begin{itemize}
    \item Examine wheather two or more DVs are related.
    \item \textbf{No IVs.}
\end{itemize}
Correlations are typically \underline{seen} in scatterplots; \underline{measured} by "r". 
\newline\newline
\textbf{Correlation does not prove causation!} The problem with correlations is what really causes what?
An example of this is the "vulnerability model" which states that people who have low self-esteem are more likely to be depressed. i.e., A causes B.
Another example is the "scar model" which states that people who are depressed are more likely to have low self-esteem. i.e., B causes A.
Finally, the "third variable" model states that there is an unknown third variable that causes both A and B. i.e., C causes A and B.
\newline\newline
To establish causality, we need three things:
\begin{itemize}
    \item Covariation
    \item Proper Sequence (cause \underline{then} effect)
    \item No confounds (\textit{Def: Any extraneous variable that changes when an IV changes})
\end{itemize}
Let's look at an example experiment. 
\sectionexample{Hot Peppers -> Mortality}
{
DV: Mortality \newline 
IV: Consumption of hot peppers (Y/N) \newline
Extraneous variables: Infinite (most are NP) \newline 
Confounds allowed: \textbf{NONE}
}
\noindent\textbf{Experiments}
\begin{itemize}
    \item Determine if changes in an IV \underline{cause} changes in a DV.
    \item \textbf{Require at least one IV.} 
\end{itemize}

\noindent Before getting into the measurements of a DV, there are two ways to manipulate an IV.

\chapterdefinition{Ways to Manipulate an IV}
{
\begin{itemize}
    \item \textbf{Between-subjects}
    \begin{itemize}
        \item Split sample into separate groups 
        \item Each group gets a different level of the IV 
        \item Comparison is \underline{between} groups
    \end{itemize}
    \item \textbf{Within-subjects} (also called "\underline{repeated measures}")
    \begin{itemize}
        \item Each member of the sample experiences each level of the IV 
        \item Comparison is \underline{within} the group
    \end{itemize}
\end{itemize}
Within-subjects gives \underline{statistical power,} but\dots gives more danger of \underline{learning effects} \newline 
\textit{Ex: testing a new teaching method\dots}
}
\subsubsection{Measuring DVs:}
\begin{itemize}
    \item "Measurement = assigning numbers to objects or events according to rules 
    \item \textit{Examples:}
    \begin{itemize}
        \item Your jersey number is 5 
        \item You graduated 5$^{th}$ in your class
        \item It is 5$^{\circ}$ fahrenheit outside
        \item The wind is 5 mph
    \end{itemize}
\end{itemize}
The following is not very interesting topic, but is crucial to remember, there are four measurement scales (from least sophisticaed to most):

\begin{enumerate}
    \item Nominal 
    \begin{itemize}
        \item For identification\dots \textit{Ex: CUID number, SSN} 
        \item \dots~or for categorical data 
        \item Good for classifying and counting\dots \textit{Ex: 5\% of CU students are psych majors}
    \end{itemize}
    \item Ordinal (put in order; ranking)
    \begin{itemize}
        \item \textit{Ex: Measuring class rank}
    \end{itemize}
    \item Interval - There are \underline{equal intervals} between values. But there is \underline{no meaningful zero} point. Cannot make any meaningful ratios. Finally, interval variables often found in \underline{surveys}.
    \begin{itemize}
        \item \textit{Ex: The temp diff between 10$^{\circ}$ and 20$^{\circ}$ is the same as diff between 90$^{\circ}$ and 100$^{\circ}$.}
        \item \textit{Ex: 0$^{\circ}$ does not mean there is no heat, so it is not a meaningful zero.}
    \end{itemize}
    \item Ratio - Equal intervals \underline{AND} a meaningful zero. Can make valid ratios. 
    \begin{itemize}
        \item \textit{Ex: Number of points you will score on exam one}
        \item \textit{Ex: Distance, time, and money}
        \item \textit{Ex: Temperature measured in $^{\circ}$ Kelvin}
        \item \textit{Ex: 60 seconds is exactly three times as long as 20 seconds}
    \end{itemize}
\end{enumerate}

\subsubsection{\underline{Examples from Clinical Psych}}
\begin{itemize}
    \item \underline{Disorders:} Mental retardation, autism, separation anxiety (Nominal)
    \item \underline{Mental Retardation Classes:} Mild, moderate, severe, profound (Ordinal)
    \item Define \underline{degree of retardation} using IQ scores (Interval)
\end{itemize}

\noindent Why are these scales important to us? Firstly, some stat. procedures \underline{depend} on the measurement scales. For example, choosing a
textbook provider, i.e., Pearson's v. Spearman's. Secondly, the scales tell us what we can do with our numbers. 
\textit{Earlier examples:}
\begin{itemize}
    \item Your jersey number is 5 (Nominal)
    \item You graduated 5$^{th}$ in your class (Ordinal)
    \item It is 5$^{\circ}$ fahrenheit outside (Interval)
    \item The wind is 5 mph (Ratio)
\end{itemize}
\newpage
\sectionexample{Driving Experiment}
{
    \textbf{The \underline{design} of the experiment:}
    \begin{itemize}
        \item Dynamic contrast sensitivity - Ability to see low contrast objects.
        \item IVs 
        \begin{itemize}
            \item Alcohol (3 Levels) 
            \begin{itemize}
                \item Alcohol (target BAC = 0.10\%)
                \item Control (expecting no alcohol, given none)
                \item Placebo (expecting acohol but given none)
            \end{itemize}
            \item Target motion (2 levels) 
            \begin{itemize}
                \item Stationary targets
                \item Moving targets
            \end{itemize}
        \end{itemize}
        \item Some extraneous variables (To be controlled)
        \begin{itemize}
            \item Age 
            \item Drinking history
            \item Medical conditions
            \item Presence of other drugs
            \item Stomach contents 
            \item Many others
        \end{itemize}
        \item \underline{Methods}\dots (stated verbally, unfortunate)
    \end{itemize}
    \textbf{The \underline{results} of the experiement.}
    \begin{itemize}
        \item But alcohol \underline{severely} reduced the ability to see \underline{moving} targets.
        \item The two measures of intoxication are NOT related\dots i.e., breath alcohol content (\%) and perceived intoxication level.
    \end{itemize}
    \textbf{The \underline{conclusion} of the experiment.}
    \begin{itemize}
        \item Moderate doses of alcohol (IV) severely reduce our ability to see low contrast moving targets (the DV)
    \end{itemize}
}

\section{Frequeny Distributions \& Percentiles}

With continuous data you can build a frequency distribution based on intervals.
\chapterdefinition{Frequency Distribution}{A table showing the number of scores in each category or interval.}
\subsubsection{Building a frequency distribution:}
\begin{itemize}
    \item Find the range of scores
    \begin{itemize}
        \item Range = biggest - smallest
    \end{itemize}
    \item Determine \underline{number of intervals} \& \underline{interval width}
    \begin{itemize}
        \item Usually 10 - 15 intervals 
        \item Approx. interval width: (range / number of intervals)
    \end{itemize}
    \item List the limits of each interval 
    \begin{itemize}
        \item First interval must contain lowest score
    \end{itemize}
    \item Tally the raw scores into intervals
    \item Add up the tallies 
\end{itemize} 

\sectionexample{Exam scores example}
{
    \begin{enumerate}
        \item Gather data - \textit{Data: (25 Exam scores)}
        \begin{itemize}
            \item 82, 75, 88, 93, 53, 84, 87, 58, 72, 94, 69, 84, 61, 91, 64, 87, 84, 70, 76, 89, 75, 80, 73, 78, 60
        \end{itemize}
        \item Find the range and number of intervals needed.
        \begin{itemize}
            \item Range = 94 - 53 = 41
            \item Width = 2 
            \item Number of intervals needed to cover 41 = 21 (too many)
            \item How about if we try a width of 10? 
            \item Number of intervals needed to cover 41 = 5 (too few)
            \item Finally, we choose a width of 5. 
            \item Number of intervals needed to cover 41 = 9 (perfect) 
        \end{itemize}
        \item List the intervals and frequency.
        Intervals - 
        \begin{itemize}
            \item 50 - 54 
            \item 55 - 59
            \item 60 - 64
            \item 65 - 69
            \item 70 - 74
            \item 75 - 79
            \item 80 - 84
            \item 85 - 89
            \item 90 - 94
        \end{itemize}
        Frequency - 
        \begin{itemize}
            \item 1, 1, 3, 1, 3, 5, 5, 4, 3 = 25 (n)
        \end{itemize}
        \% Frequency (divide frequency by total exam scores) -
        \begin{itemize}
            \item 4, 4, 12, 4, 12, 16, 20, 16, 12 = 100
        \end{itemize}
        Cumulative Frequency -
        \begin{itemize}
            \item 1, 2, 5, 6, 9, 13, 18, 22, 25
        \end{itemize}
        Cumulative \% Frequency -
        \begin{itemize}
            \item 4, 8, 20, 24, 36, 52, 72, 88, 100
        \end{itemize}
    \end{enumerate}

    \begin{tikzpicture}
        \begin{axis}[ybar interval, ymax = 5, ymin = 0, minor y tick num = 1]
            \addplot coordinates { (52, 1) (57, 1) (62, 3) (67, 1) (72, 3) (77, 4) (82, 5) (87, 4) (92, 3) };
        \end{axis}
    \end{tikzpicture}
}

\subsubsection{Cumulative Distribution, Percentiles \& Percentile Ranks}
\sectiondefinition{Cumulative Distribution}
{
    A cumulative distribution indicates the percentage of scores that are \underline{less than or equal} to each value.
}
\sectiondefinition{Percentile Rank}
{
    The \underline{percentage} of the distribution that is below a specific score (0\% - 100\%) (y-axis)
    \newline 
    \textit{Ex:} "The percentile rank of a score of 85 is 75\%."
}
\sectiondefinition{Percentile}
{
    This is the score that exceeds a given percentage of the distribution. (x-axis)
    \newline
    \textit{Ex:} "The 40th percentile is an exam score of 75."
}

\subsubsection{Stem-and-leaf Displays}
These are typically excellent for visualizing a distribution. They're used more for 
\textit{exploring data} than for presenting to an audience. 

\section{Describing Data Using Numbers}
\subsubsection{Measures of Center}
\begin{enumerate}
    \item Mode - the most frequent value. Think of a bell curve, where the tip is the mean. Often times a single bell curve is "unimodal". 
    However, they can also be "bimodal".
    \item Median - the score in the middle of the distribution. (the 50th percentile.) 
    To find the location of the median in an ordered array, use the formula : (N + 1)/2. \textit{NOTE: That this is \underline{NOT} how you FIND the median.}
    \item Mean - $\bar{x}$ = $\frac{\sum x}{N}$. The mean is the balance point of the distribution.
\end{enumerate}

\noindent Sample mean: $\bar{x}$ = $\frac{\sum x}{n}$ (This is a \underline{statistic}, since it describes a sample.) 
\newline
Population Mean: $\mu$ = $\frac{\sum x}{N}$ (This is a \underline{parameter}, since it describes a population.)

\sectionexample{\underline{Number of sexual partners}}
{
    
    \textbf{\underline{Bottom line:} The mean can be misleading when there are extreme scores. To really understand your data, look at \underline{all three} measures of center.}
}

\subsubsection{Key Features of Means}
% try to use multicol here instead 
\begin{itemize}
    \item $\sum(x - \bar{x})$ always = 0
    \begin{itemize}
        \item 3
        \item 4
        \item 5
        \item 6
        \item 7
    \end{itemize} 
    The mean of this data set is 5. ($\bar{x}$ = 5)
    \begin{itemize}
        \item -2
        \item -1
        \item 0
        \item 1
        \item 2
    \end{itemize}
    These are the "Deviation scores"; $\sum(x - \bar{x})$ = 0.00

    \item The sum of the squared deviations $\sum(x - \bar{x})^2$ is minimized by the mean.
    \begin{itemize}
        \item 3
        \item 4
        \item 5
        \item 6
        \item 7
    \end{itemize}
    The mean is 5.00. ($\bar{x}$ = 5.00)
    \begin{itemize}
        \item -2
        \item -1
        \item 0
        \item 1
        \item 2
    \end{itemize}
    The sum of the deviation score is 0.00. ($\sum(x - \bar{x})$ = 0.00)
    \begin{itemize}
        \item 4
        \item 1
        \item 0
        \item 1
        \item 4
    \end{itemize}
    The sum of the squared deviations is 10.00. ($\sum(x - \bar{x})^2$ = 10.00)
    \item Try another value where $\bar{x}$ = 4
    \begin{itemize}
        \item 1
        \item 0
        \item 1
        \item 4
        \item 9
    \end{itemize}
    The sum of the squared deviations is 15.00. ($\sum(x - \bar{x})^2$ = 15.00)
\end{itemize}

Only the \underline{mean} keeps the seesaw balanced! That's because $\sum(x - \bar{x})$ = 0.00. 
For skewed distributions, the \underline{median} is most useful.

\section{Measures of Spread}

\subsection{Three Most Common Measures of Spread}
These are going from least to most sophisticated ways of measuring spread.
\begin{itemize}
    \item Range = Largest - Smallest
    \item Variance = $\frac{\sum(x - \bar{x})^2}{N}$. \textit{Ex: }
    \begin{multicols}{3}
        The following are X
        \begin{itemize}
            \item 3
            \item 4
            \item 5
            \item 6
            \item 7
        \end{itemize}
        $\bar{x}$ = 5.00

        \columnbreak
        The following are X - $\bar{x}$
        \begin{itemize}
            \item -2
            \item -1
            \item 0
            \item 1
            \item 2
        \end{itemize}
        $\sum(X - \bar{x})$ = 0.00

        \columnbreak
        The following is $(X - \bar{x})^2$
        \begin{itemize}
            \item 4
            \item 1
            \item 0
            \item 1
            \item 4
        \end{itemize}
        $\sum(X - \bar{x})^2$ = 10.00 \textit{Note: This is also called sum of the squared deviations, or "sum of squares" or "SS"}
    \end{multicols}
    In the end, the variance is 2.00.
    \item Standard Deviation = $\sqrt{Variance}$. \textit{Ex: } $\sqrt{2.00}$ = 1.41. \textbf{Memorize this for the exam.}
\end{itemize}

\noindent Standard Deviation = $S_x = \sqrt{\frac{\sum(x - \bar{x})^2}{N}}$
\newline
Population Standard Deviation = $\sigma_x = \sqrt{\frac{\sum(x - \mu)^2}{N}}$
\newline 
Estimated Population Standard Deviation = $s_x = \sqrt{\frac{\sum(x - \bar{x})^2}{n-1}}$
\newline\newline
The first two are used when \underline{describing} data. The third is used when making \underline{inferences}. All three of these are "center-based" measures of spread. 

\sectionexample{Number Of times Rated}
{
    \begin{multicols}{3}
        The following is X
        \begin{itemize}
            \item 0
            \item 5
            \item 3
            \item 0
            \item 2
            \item 4
            \item 5
            \item 6
        \end{itemize}
        $\sum x$ = 25. $\bar{x}$ = 3.12.

        \columnbreak
        The following is X - $\bar{x}$
        \begin{itemize}
            \item -3.12
            \item 1.87
            \item -.13
            \item -3.12
            \item -1.13
            \item .87
            \item 1.87
            \item 2.87
        \end{itemize}
        $\sum(X - \bar{x})$ = 0.00

        \columnbreak
        The following is $(X - \bar{x})^2$
        \begin{itemize}
            \item 9.50
            \item 3.50
            \item .02
            \item 9.80
            \item 1.28
            \item .76
            \item 3.50
            \item 8.24
        \end{itemize}
        $\sum(X - \bar{x})^2$ = 36.88
    \end{multicols}
    Finally, using the standard deviation formula of $S_x = \sqrt{\frac{\sum(x - \bar{x})^2}{N}}$, we get a standard deviation of 2.15
}

\section{Know your Stat Calculator!}
\begin{itemize}
    \item Be sure you are familiar with its stat functions.
    \begin{itemize}
        \item Esp. standard deviation and variance
        \item (N) or (N-1) on denominator?
        \item \underline{Easy check:}
        \begin{enumerate}
            \item Enter these values: {1, 2, 3}
            \item std. dev = .82 when using N in denominator (descriptive)
            \item std. dev = 1.00 when using N-1 in denominator (inferential)
        \end{enumerate}
    \end{itemize}
    \item Got a TI-30Xa? Then watch the video that was posted by Dr. Tyrell in Canvas.
\end{itemize}

\section{Complete Descriptions of Data}
\begin{itemize}
    \item To \underline{completely} describe a variable, describe its:
    \begin{enumerate}
        \item Center 
        \begin{itemize}
            \item Mean, median, mode
        \end{itemize}
        \item Spread
        \begin{itemize}
            \item Range, standard deviation, variance
        \end{itemize}
        \item Form 
    \end{enumerate}
\end{itemize}
We've talked about the first two but what about \underline{form}?

\subsection{Measures of Form}
\begin{enumerate}
    \item Modality - The number of peaks in a distribution. \textit{Ex: Unimodal or bimodal.}
    \item Skewness - The degree to which a distribution is asymmmetrical. \textit{Ex: Positively (right \& (mean < median < mode)) or negatively (left \& (mode < median < mean)) skewed.} 
    \item Kurtosis - The degree to which a distrbution is peaked or flat. \textit{Ex: Leptokurtic (peaked) or platykurtic (flat).}
\end{enumerate}
And these are great, however the best way to see the form of a variable is to use a histogram.

\section{Transformations}

\underline{Effect of transformations on the CENTER:}
\begin{enumerate}
    \item Adding or subtracting a constant from all data points has same effect on center. 
    \item Multiplying or dividing all data points by a constant has same effect on center.
\end{enumerate}
\underline{Effect of transformations on the SPREAD:}
\begin{enumerate}
    \item Adding or subtracting a constant has NO effect on spread.
    \item Multiplying or dividing by a constant has same effect on measures of spread (\underline{except the variance!})
    \item Multiplying or dividing a constant multiplies or divides the variance by the \underline{square} of the constant.
\end{enumerate}
\sectionexample{Example of transformation}
{
    \begin{multicols}{3}
        The following is X
        \begin{itemize}
            \item 1
            \item 10
            \item 15
        \end{itemize}
        \begin{itemize}
            \item 8.67 (Mean)
            \item 5.79 (Standard Deviation)
            \item 33.56 (Variance)
        \end{itemize}
        \columnbreak
        plus 10
        \columnbreak
        The following is X + 10
        \begin{itemize}
            \item 11
            \item 20
            \item 25
        \end{itemize}
        \begin{itemize}
            \item 18.67 (Mean)
            \item 5.79 (Standard Deviation [unchanged])
            \item 33.56 (Variance [unchanged])
        \end{itemize}
    \end{multicols}
}
\sectionexample{Example of transformation}
{
    \begin{multicols}{3}
        The following is X
        \begin{itemize}
            \item 1
            \item 10
            \item 15
        \end{itemize}
        \begin{itemize}
            \item 8.67 (Mean)
            \item 5.79 (Standard Deviation)
            \item 33.56 (Variance)
        \end{itemize}
        \columnbreak
        times 10
        \columnbreak
        The following is X * 10
        \begin{itemize}
            \item 10
            \item 100
            \item 150
        \end{itemize}
        \begin{itemize}
            \item 86.67 (Mean [multiplied by 10])
            \item 57.93 (Standard Deviation [multiplied by 10])
            \item 3356.00 (Variance [multiplied by 100])
        \end{itemize}
    \end{multicols}
}

\chapter{Exam 2}

\section{z-scores and normal curves}
\sectionexample{z-scores}
{
    \begin{multicols}{3}
        The following are X
        \begin{itemize}
            \item 17
            \item 1824
            \item 27
            \item 32
        \end{itemize}
        The mean: 23.60. 
        Standard Deviation: 5.61
        \columnbreak
        The following is X - $\bar{x}$
        \begin{itemize}
            \item -6.6
            \item -5.6
            \item .4
            \item 3.4
            \item 8.4
        \end{itemize}
        Mean: 0.00 
        Standard Deviaton: 5.61 
        \columnbreak
        The following is $\frac{X - \bar{x}}{s}$
        \begin{itemize}
            \item -1.18
            \item -1.00
            \item .07
            \item .61
            \item 1.49
        \end{itemize}
        These are the z-scores. 
        Mean: 0.00
        Standard Deviation: 1.00
    \end{multicols}
}

\subsection{5 Characteristics of the Normal Curve}
\begin{enumerate}
    \item Unimodal
    \item Symmetrical
    \item Bell-shaped
    \item Asymptotic
    \item The "66-95-99.7" rule (Emperical rule)
\end{enumerate}

\subsection{3 Characteristics of z-scores}
\begin{enumerate}
    \item Mean of all z scores always = 0
    \item Std dev of all z scors always = 1
    \item z distribution always has same shape as original distribution
\end{enumerate}

\subsection{3 advantages of z-scores}
\begin{itemize}
    \item z-scores do not ignore the distance between scores (percentiles do)
    \item z-scores allow you to compare scores from different distributions
    \item If you have (or assume) a normal distribution, z-scores allow you to calculate the proportion of scores that fal between \underline{any} given range of scores.
\end{itemize}

\noindent\underline{"Standard Normal Curve"} A normal distribution with a mean of 0 and a standard deviation of 1. Allows us to translate \underline{any} normal distribution into the \underline{standard normal curve}. $Z = \frac{X - \bar{x}}{s_x}$
\newline\newline
\noindent On page 422 there is a table C.1 that shows the proportion of scores that fall between any given range of z-scores. 

\subsection{Z-score problems}
\sectionexample{What percentage of IQ scores are > 100?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

$Z = \frac{X - \bar{x}}{s}$ = $\frac{100 - 100}{15}$ = 0.00. \newline
The answer to this is 50\%.
}

\sectionexample{What percentage of IQ scores are greater than 105?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline 

$Z = \frac{X - \bar{x}}{s}$ = $\frac{105 - 100}{15}$ = 0.33. \newline
The answer would be 37.07\%.
}


\sectionexample{What proportion of IQ scores are less than 90?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline 
Z: -2, -1, 0, 1, 2 \newline

We want to look at the IQ scores of 90 and less. 
$Z = \frac{X - \bar{x}}{s}$ = $\frac{90 - 100}{15}$ = -0.67. \newline
The answer would be .2514.
}

\sectionexample{What percentage of IQ scores are > 87}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130  \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the IQ scores of 87 and above.
$Z = \frac{X - \bar{x}}{s}$ = $\frac{87 - 100}{15}$ = -0.87. \newline
The answer would be .3078 + .5000 = .8078 which is 80.78\%.
}

\sectionexample{What proportion of IQs are between 112 and 130?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the IQ scores between 112 and 130. \newline
$Z = \frac{X - \bar{x}}{s}$ = $\frac{130 - 100}{15}$ = 2.00. \newline
$Z = \frac{X - \bar{x}}{s}$ = $\frac{112 - 100}{15}$ = 0.80. \newline

From the table, we can see that 2.00 = .4772 and .8000 = .2881 We then want to subtract the biggest number and the smallest to get our proportion. \newline
.4772 - .2881 = .1891.
}

\sectionexample{What is the percentage of IQ scores between 0.5 and 1.5 standard deviations above the mean?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the IQ scores between 0.5 and 1.5 standard deviations above the mean. \newline
1.5 = .4332. \newline
.5 = .1915. \newline
.4332 - .1915 = .2417 which is 25.17\% \newline 
}

\sectionexample{What z-score corresponds to the $40^{th}$ percentile?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the bottom 40\% of the data from the upper 60\%. \newline
$Z = \frac{X - \bar{x}}{s}$ = $\frac{X - 100}{15}$ = -0.255. \newline
}

\sectionexample{What IQ score corresponds to the 25th percentile?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the bottom 25\% of the data from the upper 75\%. \newline
$Z = \frac{X - \bar{x}}{s}$ = $\frac{X - 100}{15}$ = -0.675. \newline
Split two numbers from the textbook column in half to get this Z score. \newline
Now, to find the IQ score we do the following: \newline

$X = \bar{x} + Zs$ = $100 + (-0.675)(15)$ = 89.875. \newline
}

\sectionexample{What 2 z-scores surround the central 95\% of a normal distribution?}
{
Where the std is 15 and the mean is 100. \newline
IQ: 70, 85, 100, 115, 130 \newline
Z: -2, -1, 0, 1, 2 \newline

We want to look at the central 95\% of the data. \newline
Split the 95\% in half to get 47.5\% on each side. \newline
From the table, we can see that 1.96 = .4750. \newline
The two z-scores are -1.96 and 1.96. \newline
}

\section{Correlations}
\subsection{Scatterplots}
\textit{Note:} The x-axis is called the abscissa and the y-axis is called the ordinate. \newline 
\begin{itemize}
    \item Strong \underline{positive} relationship. When X goes up, Y goes up.
    \item Strong \underline{negative} relationship. When X goes up, Y goes down.
\end{itemize}

\subsection{Correlation Coefficient}
\begin{itemize}
    \item A single number that describes the strength of the linear relationship betwene two variables. 
    \item Value is always between -1 and +1. A perfect negative correlation is -1 and a perfect positive correlation is +1. A 0 means no correlation.
\end{itemize}

There are two different kinds of correlation coefficients. 
\begin{itemize}
    \item Pearson's product-moment correlation coefficient (r)
    \item Spearman's rank-order correlation coefficient ($r_s$)
\end{itemize}

\subsection{Calculating Pearson's R}
\sectionexample{Example 1}
{
    \begin{multicols}{5}
        X 
        \begin{itemize}
            \item 6
            \item 9
            \item 4
            \item 2
            \item 4
        \end{itemize}
        \columnbreak
        Y
        \begin{itemize}
            \item 7
            \item 4
            \item 6
            \item 10
            \item 3
        \end{itemize}
        \columnbreak
        X - $\bar{x}$
        \begin{itemize}
            \item 1
            \item 4
            \item -1
            \item -3
            \item -1
        \end{itemize}
        \columnbreak
        Y - $\bar{y}$
        \begin{itemize}
            \item -3
            \item 4
            \item 1
            \item -2
            \item 0
        \end{itemize}
        \columnbreak
        $(X - \bar{x})(Y - \bar{y})$
        \begin{itemize}
            \item -3
            \item 16
            \item -1
            \item 6
            \item 0
        \end{itemize}
        $\sum(X - \bar{x})(Y - \bar{y})$ = 18.00

    \end{multicols}
    Mean of X = 5.00. \newline
    Mean of Y = 3.00. \newline
    S of X = 2.37. \newline
    S of Y = 2.45. \newline

    \underline{Pearson's r} = $\frac{cov(x, 4)}{s_x s_y}$. \newline 
    Where cov(x, y) = $\frac{\sum(X - \bar{x})(Y - \bar{y})}{N}$ = $\frac{18}{5} = 3.6$ \newline
    \textit{Note: N is the number of pairs of observations} \newline
    \textit{Note: Covariance and correlation are always the same sign.} \newline 
    \textit{Note: Covariance is big when variance is big.} \newline\newline 

    $r = \frac{cov(x, y)}{s_x s_y} = \frac{3.6}{2.37 * 2.45}$ = +0.62. \newline
}

\sectionexample{Example 2}
{
    Let's try a new but same equation. $r = \frac{cov(x, y)}{s_x s_y} = \frac{\sum(z_x z_y)}{N}$. \newline
    Using the same previous data, we can find Z. \newline 
    \begin{multicols}{3}
        Z of X
        \begin{itemize}
            \item .42
            \item 1.69
            \item -.42
            \item -1.27
            \item -.42
        \end{itemize}
        \columnbreak
        Z of Y
        \begin{itemize}
            \item -1.22
            \item 1.63
            \item .41
            \item -0.82
            \item 0
        \end{itemize}
        \columnbreak
        Z of X * Z of Y
        \begin{itemize}
            \item -.512
            \item 2.755
            \item -.172
            \item 1.041
            \item 0
        \end{itemize}
        $\sum(Z_x Z_y)$ = 3.112
    \end{multicols}
    $r = \frac{cov(x, y)}{s_x s_y} = \frac{3.112}{5} = 0.62$. \newline
    \textit{Note: Pearson's r is the "average correspondance between pairs of z-scores".}
}

\subsection{Three ways to calculate Pearson's r}
\subsection{Pearson's vs Spearman's}
\begin{itemize}
    \item Pearson's r only applies when both variables are either \underline{interval} or \underline{ratio} measurement scales. \textit{Note: It's based on difference scores!}
    \item If either variable (or both) is \underline{ordinal}, use Spearman's.
    \item There are 2 approaches to calculating Spearman's. 
    \begin{enumerate}
        \item Use the textbook's formula.
        \item Use the "Tyrrell method"
    \end{enumerate}
\end{itemize}

\subsubsection{The Tyrrell Method}
\underline{The Tyrrell approach to calculating Spearman's r}
\begin{enumerate}
    \item Convert both variables to ranks. 
    \item Apply Pearson's formulas to the ranks. 
\end{enumerate}

\section{Linear Regression}
Remember: Correlations describe the strength of the linear relationship between two variables. 
\begin{itemize}
    \item But once we've \underline{described} it, can we use it to make predictions?
    \item \textit{Ex:} Predicting weight\dots
\end{itemize}
Regression: "Model" the relationship and use the model to make predictions. 
\begin{itemize}
    \item Our model will be based on simple linear algebra: $Y = a + bX$
    \item Where Y is the predicted value of the dependent variable, X is the value of the independent variable, a is the Y-intercept, and b is the slope of the line.
    \item If the slope is positive, the relationship is positive. If the slope is negative, the relationship is negative.
    \item Our model minimizes the sum of the squared residuals. 
    \item That is, the goal of regression is to build a model that minimizes this: $\sum (Y - Y')^2$.
    \item Therefore, regression models are often called "least squares" models.
\end{itemize}

\newpage
\sectionexample{Example 1}
{
    Using the same X, Y numbers from example 1 of Calculating Pearson's R: \newline\newline
    $B = \frac{N\sum XY-(\sum X)(\sum Y)}{N\sum X^2 - (\sum X)^2}$. \newline

    \begin{multicols}{2}
        $X^2$
        \begin{itemize}
            \item 36
            \item 81
            \item 16
            \item 4
            \item 16
        \end{itemize}   
        \columnbreak
        $XY$
        \begin{itemize}
            \item 18
            \item 90
            \item 28
            \item 8
            \item 24
        \end{itemize}
    \end{multicols}

    $b = \frac{5(168) - (25)(168)}{5(153) - (25)^2}$ = $\frac{840 - 750}{765 - 625}$ = .64. \newline\newline
    Y = 0.64X \newline
    
    Or, $b = r(S_y/S_x)$ = 0.62(2.45/2.37) = 0.64. \newline\newline
    Y = 0.64X \newline

    $a = \bar{y} - b\bar{x}$ = 6.0 = (.64)(5.0) = 2.80 \newline

    Now we have Y = 2.80 + 0.64X. \newline
    \begin{multicols}{3}
        $Y'$
        \begin{itemize}
            \item 6.64
            \item 8.56
            \item 5.36
            \item 4.08
            \item 5.36
        \end{itemize}
        \columnbreak
        $Y - Y'$
        \begin{itemize}
            \item -3.64
            \item 1.44
            \item 1.64
            \item -.08
            \item .64
        \end{itemize}
        \columnbreak
        $(Y - Y')^2$
        \begin{itemize}
            \item 13.25
            \item 2.07
            \item 2.69
            \item .01
            \item .41
        \end{itemize}
        $\sum(Y - Y')^2$ = 18.43
    \end{multicols}

    $S_y' = \sqrt{\frac{\sum(Y - Y')^2}{N}}$ = $\frac{\sqrt{18.43}}{5}$ = 1.92 \newline
}

Residual = prediction error = observed - predicted. \newline\newline 

\underline{Think of the regression line as:}
\begin{itemize}
    \item The regression model.
    \item All predicted values connected together.
    \item The 2-dimensional \underline{center} of the data.
\end{itemize}
\underline{Think of the standard devation error of the estimate $S_y$ as:}
\begin{itemize}
    \item A 2-dimensional measure of \underline{spread}.
    \item "average error" of model
\end{itemize}
\underline{Strong correlations give:}
\begin{itemize}
    \item Strong models
    \item Small residuals
    \item Small $S_(y')$
\end{itemize}
\underline{Weak correlations give:}
\begin{itemize}
    \item Weak models
    \item Big residuals
    \item Big $S_(y')$
\end{itemize}

\sectionexample{Regression Example: Handwriting \& Personality}
{
    Specifically: Can extraversion be predicted by loop width? \newline 
    Where extraversion is a personality characteristic and width (in mm) of the loops in the letters a, p, and d. \newline
}

\subsubsection{\underline{"Proportion of variance accounted for"}}

Using $\bar{Y}$ to predict Y scores results in lots of error. However, using the known relationship to predict Y scores results in less error. But \underline{how much} less? \newline

\subsubsection{\underline{$r^2$ measures "predictable variability"}}

(Remember: r is the correlation coefficient.)\newline 
$r^2$ tells us \underline{the proportion of variance in Y that can be predicted in X}. \newline

\begin{multicols}{3}
    \noindent If r = .20, $r^2$ = .04. X predicts 4\% of the variance of Y.
    \columnbreak

    If r = .50, $r^2$ = .25. X predicts 25\% of the variance of Y. 
    \columnbreak
    
    If r = .90, $r^2$ = .81. X predicts 81\% of the variance of Y.
\end{multicols}
\noindent $r^2$ is the best way to evaluate the scientific \underline{importance} of a relationship. 

\section{Probability}

\subsection{Three probability rules}
\begin{enumerate}
    \item When outcomes are equally ikely, the probability of an event is: p = $\frac{number of possibilities of that event}{total number of possibilities}$
    \item This is called the "additive rule". The probability of \underline{either} of two mutually eclusive events occurring is the \underline{sum} of their separate probabilities.
    \item This is called the "multiplicative rule". The probability of two independent events \underline{both} occuring is the \underline{product} of their separate probabilities.
\end{enumerate}

\underline{Notice that:} Using the additive rule $\uparrow$ probability, but using the multiplicative rule $\downarrow$ probability. Apply this to the simple lottery\dots

\sectionexample{Powerball Math}
{
    \begin{itemize}
        \item Odds of winning the big jackpot:
        \begin{itemize}
            \item 1 in 292,201,338
        \end{itemize}
        \item \underline{If you buy 25 tickets/week (@ \$2):}
        \begin{itemize}
            \item You can expect to win once every 225,000 years
        \end{itemize}
        \item A lottery is a "zero-sum game."
    \end{itemize}
}

\sectiondefinition{Bernoulli's Theorem "The law of large numbers"}
{
    Predicts that as N gets larger the frequency of an event will approach (p*N). \newline 

    In other words\dots As N increases, we can expect the frequnecies to be closer and closer to the predicted frequencies.
}

\section{Sampling Distribution}

\sectionexample{Example 1}
{
    The population: \newline 
    Body weights of all 30 students in a class. \newline
    $\mu = 139.9$ \newline
    $N = 30$ \newline

    Let's pick a few samples, each of size n = 2\dots (n is sample size, N is population size) \newline\newline 
    \underline{Every possible sample mean:} Sample means will fluctuate, even though population means do not. \newline\newline 
}

\sectiondefinition{Sampling Distribution}
{
    A distribution of the probabilities of \underline{all possible values} of a statistic. \textit{These are important because they tell us how much sample-to-sample variability we should expect in our statistic.}

    This is represented by the "sampling distribution of every possible sample mean." \newline 
    $\mu_{\bar{x}} = $ "Mean of all the x bar" \newline
    $\sigma_{\bar{x}} = $ "Standard deviation (error) of the x bars" \newline
}

\subsection{Standard Error of the Mean}
\begin{itemize}
    \item The S.E.M is the standard deviation of a sampling distribution.
    \item "How spread out are the sample means?" or "How much error can you expect to be in your sample mean?"
    \item The S.E.M depends on only 2 factors: 
    \begin{enumerate}
        \item As $\sigma$ increases, the S.E.M increases.
        \item As n increases, the S.E.M decreases.
    \end{enumerate}
    \item Formula for S.E.M: $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}}$ {\textit{Note: $\sigma$ is the standard deviation of the population.} \textbf{THIS IS ANOTHER FORMULA TO MEMORIZE}}
\end{itemize}

\subsubsection{How can we fully describe a sampling distribution?} 
\begin{itemize}
    \item We'll \underline{rarely} take every possible sample from a population. 
    \begin{itemize}
        \item Normally, we only take one!
    \end{itemize}
    \item So, we need a system of shortcuts tto allow us to describe a sampling distribution. 
\end{itemize}

\subsection{Central Limit Theorem}
\begin{itemize}
    \item Center: Mean of sampling distribution = mean of the population. $\mu_{\bar{x}} = \mu$
    \item Spread: Standard deviation of sampling distribution = S.E.M.: $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}}$
    \item Form: "As n increases, sampling distribution becomes more normal." Also, if the population is normal, then the S.D is as well (regardless of n).
\end{itemize}

\sectionexample{Central Limit Theorem In Action}
{
    \underline{Extreme Example:} Start with a population that's \underline{not} normally distributed. \textit{Ex: all numbers from 0 - 100, evenly distributed.} \newline 
    $\mu = 50$ \newline
    $\sigma = 28.9$ \newline
    Even with a small N (just 4), sampling distribution looks \underline{somewhat} normal. \newline
    $\mu_{\bar{x}} = 50$ \newline
    $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}} = \frac{28.9}{\sqrt{4}} = 14.43$ \newline
    But now, with a larger N (30), sampling distribution looks \underline{very} normal. \newline
    $\mu_{\bar{x}} = 50$ \newline
    $\sigma_{\bar{x}} = \frac{\sigma}{\sqrt{N}} = \frac{28.9}{\sqrt{30}} = 5.27$ \newline
    Remember: C.L.T says that as N increases the sampling distribution becomes normal and the S.E.M shrinks. \newline\newline

    What these look like visually, the smaller the N, the graph tends to look more wider, but as N increases, the graph tends to look more normal and skinnier.
}

\subsection{\underline{Using} Sampling Distributions}
What if we take a sample from this population and our $\bar{x}$ is 160? Maybe our mean is from a \underline{different} population?
How far from 139 can our $\bar{x}$ get before we decide something weird is going on? In order to answer questions like this we will have to rely on three tools. \newline 
\begin{enumerate}
    \item Probability (three rules and a low)
    \item Knowledge of sampling distributions
    \item Logic of \underline{hypothesis testing}
\end{enumerate}

\section{Hypothesis Testing}

\sectionexample{Example of Hypothesis Testing}
{
    Your particular interest is in behavior problems in stressed children. Mean beahvior problems score of general population is 50. 
    We have access to five children whose parents are newly divorced. Their mean is 53. \newline\newline 
    Can we conclude that stressed children exhibit an increased number of behavior problems? 
    Is the +3 difference due to a \underline{real effect} of stress or is it due to \underline{sampling error}? \newline\newline 

    The distribution of all possible sample means of behavior problems, each based on n = 5 scores. This shows as a normal distribution of a simpling distribution on a histogram. \newline\newline 

    What's the probability of getting a smple mean of 53 if you're sampling from a population with a mean of 50? \newline\newline 

    The first kind of hypothesis testnig we'll do is the easist\dots: The z-test.
}

When using the z-test, we can take interest in answering the following question. "Does this particlar sample come from a poulation with a mean of [insert hypothesized value of $\mu$ here]?" \newline

\subsection{Seven steps involved in hypothesis testing}

\begin{enumerate}
    \item Define the null hypothesis, $H_0$.
    \begin{itemize}
        \item Statement of the parameter value that we want to test. \textit{Note: Must state a specific parameter and give it a value.}
        \item Example: $H_0: \mu = 50$
        \item Usually asserts that a more interesting hypothesis is wrong. 
        \item $H_0$ is the hypothesis that we actually test. 
    \end{itemize}
    \item Specify the alternative hypothesis, $H_a$ (also called the research hypothesis).
    \begin{itemize}
        \item Logical opposite to the null hypothesis.
        \item $H_a$ and $H_0$ must be mutually exclusive and exhaustive.
        \item $H_a$ is usually the hypothesis we hope to support. 
        \item Example: $H_a: \mu \neq 50$
    \end{itemize}
    \item Construct the sampling distribution under the assumption that the null hypothesis is true.
    \begin{itemize}
        \item What would be the \underline{center, spread, and form} of the sampling distribution if $H_0$ were true?
        \begin{itemize}
            \item Center usually specified by $H_0$.
            \item Spread = standard error of the mean. 
            \item Form: Is it normal? 
        \end{itemize}
    \end{itemize}
    \item Collect the Data
    \item Compare the sample statistic to the value in $H_0$.
    \item \underline{Find the probability} of exceeding the observed statistic's value.
    \item Make the inference. 
    \begin{itemize}
        \item Decide which decision to make\dots only two possibilities. 
        \item Either \textbf{reject $H_0$} (if it is \underline{unlikely} to be true)
        \item or \textbf{fail to reject $H_0$} (if it is \underline{likely} to be true)
    \end{itemize}
\end{enumerate}
\begin{itemize}
    \item Remember, this logic is indirect:
    \begin{itemize}
        \item We're testing the hypothesis that we hope is wrong ($H_0$).
        \item If we reject $H_0$, we've found indirect evidence to support $H_a$.
    \end{itemize}
\end{itemize}

\subsubsection{New form of Z}

$Z_{calc} = \frac{\bar{X} - \mu}{\sigma_{\bar{x}}} \leftarrow$ "How many standard error of the means is my sample mean from the $H_a$ value?" \newline\newline 

If $\abs{Z_{calc}} > Z_{critical}$, then reject $H_0$. \newline
Fail to rject because (finish this its in iphone photos)


\subsection{Errors in Making Inferences} 

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        & \textbf{True $H_0$} & \textbf{False $H_0$} \\
        \hline
        \textbf{Fail to reject $H_0$} & Correct decision $p = 1 - \alpha$ & Type II error (keeping a false $H_0$) $p = \beta$\\
        \hline
        \textbf{Reject $H_0$} & Type I error (rejecting a true $H_0$) $p = \alpha$ & Correct decision $p = 1 - \beta$ "power" \\
        \hline
    \end{tabular}
    \caption{Table of our inference and actual state of nature}
\end{table}

\subsubsection{Statistical Power}
\begin{itemize}
    \item The probability of correctly rejecting a false null hypothesis. 
    \item $p = 1 - \beta$
\end{itemize}

\subsubsection{Power is influenced by four things:}
\begin{enumerate}
    \item $\mu$ (father from $H_O$, the more power)
    \item $\sigma$ (smaller the sigma the more power) 
    \item $\alpha$ (bigger the alpha the more power)
    \item N (more data the more power) 
    \item \textit{Note: Number 3 and 4 are within your control.}
\end{enumerate}

\subsection{Why is it good to get more data?}
\begin{itemize}
    \item Because as we get $\uparrow N$, we get \underline{more power}
    \item But how? 
    \item $\uparrow N \rightarrow \downarrow \sigma_{\bar{x}} \rightarrow \uparrow Z_{calc} \rightarrow$ easier to reject a false $H_O$ (this is what "more power" means).
\end{itemize}

\section{References}
This is a reference to a source \cite{example}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}